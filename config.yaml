
model_list:
  - model_name: vault-gemini-flash-lite
    litellm_params:
      model: gemini/gemini-flash-lite-latest
      api_key: ${GEMINI_API_KEY}
      api_base: null
      max_tokens: 4096
      temperature: 0.7

  - model_name: vault-gemini-2.5-pro
    litellm_params:
      model: gemini/gemini-2.5-pro
      api_key: ${GEMINI_API_KEY}
      api_base: null
      max_tokens: 4096
      temperature: 0.7

  - model_name: vault-haiku-3
    litellm_params:
      model: claude-3-haiku-20240307
      api_key: ${ANTHROPIC_API_KEY}
      api_base: null
      max_tokens: 4096
      temperature: 0.7

  - model_name: vault-gpt-4o-mini
    litellm_params:
      model: gpt-4o-mini
      api_key: ${OPENAI_API_KEY}
      api_base: null
      max_tokens: 4096
      temperature: 0.7

  - model_name: vault-mistral-nemo-8k
    litellm_params:
      model: ollama/nemo-8k:latest
      api_base: ${OLLAMA_BASE_URL}
      api_key: null
      max_tokens: 4096
      temperature: 0.7

  - model_name: vault-qwen3-8k
    litellm_params:
      model: ollama/qwen3-8k:latest
      api_base: ${OLLAMA_BASE_URL}
      api_key: null
      max_tokens: 8192
      temperature: 0.7

  - model_name: vault-deepseek-coder
    litellm_params:
      model: deepseek/deepseek-coder
      api_key: ${DEEPSEEK_API_KEY}
      api_base: null
      max_tokens: 4096
      temperature: 0.7

  - model_name: vault-grok-code-fast-1
    litellm_params:
      model: xai/grok-code-fast-1
      api_key: ${XAI_API_KEY}
      api_base: null
      max_tokens: 4096
      temperature: 0.7

# General settings
general_settings:
  # Master key for accessing all models
  master_key: ${CHATVAULT_API_KEY}

  # Database settings
  database_url: ${DATABASE_URL}

  # Logging
  log_level: ${LOG_LEVEL}

  # Telemetry (disable for privacy)
  telemetry: false

# Router settings
router_settings:
  # Routing strategy (simple, usage-based, latency-based)
  routing_strategy: "simple"

  # Load balancing configuration
  load_balancing:
    # Default algorithm: round_robin, least_loaded, random, weighted_round_robin
    algorithm: "round_robin"

    # Health check settings
    health_check_interval: 60  # seconds
    health_check_timeout: 10   # seconds

    # Circuit breaker settings
    circuit_breaker_failures: 3    # consecutive failures to trigger
    circuit_breaker_timeout: 300   # seconds to wait before retry

    # Per-model instance configuration (optional)
    # Uncomment and configure for multiple instances per model
    instances:
      # vault-gpt-4o-mini:
      #   - id: "openai-primary"
      #     base_url: null
      #     api_key: ${OPENAI_API_KEY}
      #     weight: 2
      #     max_concurrent: 20
      #     timeout: 30
      #   - id: "openai-secondary"
      #     base_url: null
      #     api_key: ${OPENAI_API_KEY_2}
      #     weight: 1
      #     max_concurrent: 15
      #     timeout: 30

  # Fallback settings
  fallbacks:
    - vault-architect: ["vault-architect-3-5"]
    - vault-local: ["vault-local-large"]

  # Rate limiting per user
  rate_limit_per_user:
    requests_per_minute: ${RATE_LIMIT_REQUESTS}

  # Model aliases for easier access
  model_aliases:
    architect: vault-architect
    local: vault-local
    deepseek: vault-deepseek

# Cost calculation settings
cost_calculation:
  # Enable advanced cost features
  enable_advanced_costs: true

  # Currency for cost calculations (ISO 4217 code)
  currency: "USD"

  # Exchange rate settings (for multi-currency support)
  exchange_rates:
    USD: 1.0
    EUR: 0.85
    GBP: 0.73

  # Custom cost definitions for models not supported by LiteLLM built-in pricing
  custom_costs:
    "ollama/nemo-8k:latest":
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
      max_tokens: 4096
      currency: "USD"
    "ollama/qwen3-8k:latest":
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
      max_tokens: 8192
      currency: "USD"

  # Tiered pricing support (volume discounts)
  tiered_pricing:
    enabled: true
    tiers:
      - name: "starter"
        monthly_token_limit: 100000
        discount_percentage: 0.0
        models: ["*"]  # All models
      - name: "professional"
        monthly_token_limit: 1000000
        discount_percentage: 10.0
        models: ["*"]
      - name: "enterprise"
        monthly_token_limit: 10000000
        discount_percentage: 20.0
        models: ["*"]

  # Cost prediction settings
  cost_prediction:
    enabled: true
    # Historical data window for prediction (days)
    prediction_window_days: 30
    # Confidence interval for predictions
    confidence_interval: 0.95

  # Budget and alerting settings
  budget_alerts:
    enabled: true
    # Global budget limits
    global_limits:
      daily_budget: 100.0
      monthly_budget: 3000.0
      currency: "USD"

    # Per-user budget limits
    user_limits:
      enabled: true
      default_daily_limit: 10.0
      default_monthly_limit: 300.0

    # Alert thresholds (percentage of budget)
    alert_thresholds: [50, 80, 90, 95, 100]

    # Alert channels
    notifications:
      email:
        enabled: false
        smtp_server: "smtp.gmail.com"
        smtp_port: 587
        sender_email: "alerts@chatvault.com"
      webhook:
        enabled: false
        url: "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
      log:
        enabled: true
        level: "WARNING"

  # Cost reporting settings
  reporting:
    enabled: true
    # Default reporting period
    default_period: "monthly"
    # Available periods
    available_periods: ["daily", "weekly", "monthly", "yearly"]
    # Cost breakdown categories
    breakdown_categories:
      - "model_usage"
      - "provider_fees"
      - "platform_fees"
      - "discounts"

# CLI Client definitions for command-line usage
clients:
  local1:
    bearer_token: "YOUR_LOCAL1_BEARER_TOKEN"
    allowed_models:
      - "vault-qwen3-8k"  # Restricted to local Ollama models
      - "vault-mistral-nemo-8k"

  full1:
    bearer_token: "YOUR_FULL1_BEARER_TOKEN"
    allowed_models:
      - "*"  # Allow all models