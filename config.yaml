model_list:
  - model_name: vault-gemini-flash-lite
    litellm_params:
      model: gemini/gemini-flash-lite-latest
      api_key: ${GEMINI_API_KEY}
      api_base: null
      max_tokens: 4096
      temperature: 0.7

  - model_name: vault-haiku-3
    litellm_params:
      model: claude-3-haiku-20240307
      api_key: ${ANTHROPIC_API_KEY}
      api_base: null
      max_tokens: 4096
      temperature: 0.7

  - model_name: vault-gpt-4o-mini
    litellm_params:
      model: gpt-4o-mini
      api_key: ${OPENAI_API_KEY}
      api_base: null
      max_tokens: 4096
      temperature: 0.7

  - model_name: vault-mistral-nemo-8k
    litellm_params:
      model: ollama/nemo-8k:latest
      api_base: ${OLLAMA_BASE_URL}
      api_key: null
      max_tokens: 4096
      temperature: 0.7

  - model_name: vault-qwen3-8k
    litellm_params:
      model: ollama/qwen3-8k:latest
      api_base: ${OLLAMA_BASE_URL}
      api_key: null
      max_tokens: 8192
      temperature: 0.7

  - model_name: vault-deepseek-coder
    litellm_params:
      model: deepseek/deepseek-coder
      api_key: ${DEEPSEEK_API_KEY}
      api_base: null
      max_tokens: 4096
      temperature: 0.7

  - model_name: vault-grok-code-fast-1
    litellm_params:
      model: xai/grok-code-fast-1
      api_key: ${XAI_API_KEY}
      api_base: null
      max_tokens: 4096
      temperature: 0.7

# General settings
general_settings:
  # Master key for accessing all models
  master_key: ${CHATVAULT_API_KEY}

  # Database settings
  database_url: ${DATABASE_URL}

  # Logging
  log_level: ${LOG_LEVEL}

  # Telemetry (disable for privacy)
  telemetry: false

# Router settings
router_settings:
  # Routing strategy (simple, usage-based, latency-based)
  routing_strategy: "simple"

  # Fallback settings
  fallbacks:
    - vault-architect: ["vault-architect-3-5"]
    - vault-local: ["vault-local-large"]

  # Rate limiting per user
  rate_limit_per_user:
    requests_per_minute: ${RATE_LIMIT_REQUESTS}

  # Model aliases for easier access
  model_aliases:
    architect: vault-architect
    local: vault-local
    deepseek: vault-deepseek

# Cost calculation settings
cost_calculation:
  # Custom cost definitions for models not supported by LiteLLM built-in pricing
  custom_costs:
    "ollama/nemo-8k:latest":
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
      max_tokens: 4096
    "ollama/qwen3-8k:latest":
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
      max_tokens: 8192

# CLI Client definitions for command-line usage
clients:
  local1:
    bearer_token: "YOUR_LOCAL1_BEARER_TOKEN"
    allowed_models:
      - "vault-qwen3-8k"  # Restricted to local Ollama models
      - "vault-mistral-nemo-8k"

  full1:
    bearer_token: "YOUR_FULL1_BEARER_TOKEN"
    allowed_models:
      - "*"  # Allow all models
